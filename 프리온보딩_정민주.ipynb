{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "프리온보딩_정민주.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNT4lcaSyiqb/ALmxpNptYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meiren13/test_project/blob/master/%ED%94%84%EB%A6%AC%EC%98%A8%EB%B3%B4%EB%94%A9_%EC%A0%95%EB%AF%BC%EC%A3%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 1) Tokenizer 생성하기"
      ],
      "metadata": {
        "id": "fG8z71qS_-rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.word_dict = {'oov': 0}\n",
        "        self.fit_checker = False\n",
        "\n",
        "    def preprocessing(self, sequences):\n",
        "        result = []\n",
        "        # 문제 1-1.\n",
        "        for sentence in sequences:\n",
        "\n",
        "            # # ver.1\n",
        "            # tmp_sentence = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentence)\n",
        "\n",
        "            # ver.2\n",
        "            specialChar = '!@#$%^&*()_{}[]\\|:<>.?/'\n",
        "            tmp_sentence = ''.join(c for c in sentence if c not in specialChar)\n",
        "           \n",
        "            tmp_sentence = tmp_sentence.lower().split()\n",
        "            result.append(tmp_sentence)\n",
        "        return result\n",
        "\n",
        "    def fit(self, sequences):\n",
        "        self.fit_checker = False\n",
        "        # 문제 1-2.\n",
        "        num = 0\n",
        "        pre_sentences = self.preprocessing(sequences)\n",
        "\n",
        "        for pre_sentence in pre_sentences:\n",
        "            for pre_word in pre_sentence:\n",
        "                if pre_word not in self.word_dict:\n",
        "                    num += 1\n",
        "                    self.word_dict[pre_word] = num\n",
        "        word_dict = self.word_dict\n",
        "        self.fit_checker = True\n",
        "        return self.word_dict   \n",
        "\n",
        "    def transform(self,sequences):\n",
        "        result = []\n",
        "        tokens = self.preprocessing(sequences)\n",
        "        \n",
        "        if self.fit_checker:\n",
        "            for token in tokens:\n",
        "                token_list = []\n",
        "                for word in token:\n",
        "                    token_list.append(self.word_dict[word])\n",
        "                result.append(token_list)\n",
        "            return result\n",
        "        else:\n",
        "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "\n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        result = self.transform(sequences)\n",
        "        return result"
      ],
      "metadata": {
        "id": "wewCTV8X0Jnf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 2) TfidfVectorizer 생성하기"
      ],
      "metadata": {
        "id": "bc_Ec5CO_6h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.fit_checker = False\n",
        "\n",
        "    def fit(self, sequences):\n",
        "        tokenized = self.tokenizer.fit_transform(sequences)\n",
        "        '''\n",
        "        문제 2-1.\n",
        "        '''\n",
        "        import math\n",
        "        idf = []\n",
        "        count_tind = {}\n",
        "\n",
        "        for sentence in tokenized:\n",
        "            sentence = set(sentence)\n",
        "            for t in sentence:\n",
        "                try:\n",
        "                    count_tind[t] += 1\n",
        "                except:\n",
        "                    count_tind[t] = 1\n",
        "\n",
        "        for d in list(count_tind.values()):\n",
        "            idf_t = math.log(n/(1 + d))\n",
        "            idf.append(idf_t)\n",
        "        self.fit_checker = True\n",
        "        return idf\n",
        "\n",
        "\n",
        "    def transform(self, sequences):\n",
        "        if self.fit_checker:\n",
        "            tokenized = self.tokenizer.transform(sequences)  # tokenizer.transform(sequences)\n",
        "            '''\n",
        "            문제 2-2.\n",
        "            '''\n",
        "            count_tofd = []\n",
        "            for sentence in tokenized:\n",
        "                tf_idf_sentence = {}\n",
        "                for t in set(sentence):\n",
        "                    tf_idf_sentence[t] = sentence.count(t)\n",
        "                count_tofd.append(tf_idf_sentence)\n",
        "\n",
        "            tfidf_matrix = []\n",
        "            for idx, sentence in enumerate(tokenized):\n",
        "                tmp_df_idf = []\n",
        "                for t in sentence:\n",
        "                    tf_idf = count_tofd[idx][t] * idf[t - 1]\n",
        "                    tmp_df_idf.append(tf_idf)\n",
        "                tfidf_matrix.append(tmp_df_idf)\n",
        "            return(tfidf_matrix)\n",
        "        else:\n",
        "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        return self.transform(sequences)"
      ],
      "metadata": {
        "id": "NqRMPhAn_6Oh"
      },
      "execution_count": 156,
      "outputs": []
    }
  ]
}